# -*- coding: utf-8 -*-
"""lab2-Adeniran.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O1fYJ4c9H-NhiB4baLG9GYXHF8MhXjZE
"""

from keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
import keras.backend as K
from keras.models import Sequential
import keras.utils as ku
import numpy as np
from matplotlib import pyplot as plt

tokenizer = Tokenizer()
max_sequence_len = 40
total_words=21

# function to split a string
def split(word):
    return [char for char in word]

# function to represent each character by a number
def charToNum(ch):
    if ch == '0':
        num = 0
    elif ch == 'A':
        num = 1
    elif ch == 'C':
        num = 2
    elif ch == 'D':
        num = 3
    elif ch == 'E':
        num = 4
    elif ch == 'F':
        num = 5
    elif ch == 'G':
        num = 6
    elif ch == 'H':
        num = 7
    elif ch == 'I':
        num = 8
    elif ch == 'K':
        num = 9
    elif ch == 'L':
        num = 10
    elif ch == 'M':
        num = 11
    elif ch == 'N':
        num = 12
    elif ch == 'P':
        num = 13
    elif ch == 'Q':
        num = 14
    elif ch == 'R':
        num = 15
    elif ch == 'S':
        num = 16
    elif ch == 'T':
        num = 17
    elif ch == 'V':
        num = 18
    elif ch == 'W':
        num = 19
    elif ch == 'Y':
        num = 20

    return num

# function to convert a number to its corresponding letter/character
def NumToChar(num):
    if num == 0:
        ch = '0'
    elif num == 1:
        ch = 'A'
    elif num == 2:
        ch = 'C'
    elif num == 3:
        ch = 'D'
    elif num == 4:
        ch = 'E'
    elif num == 5:
        ch = 'F'
    elif num == 6:
        ch = 'G'
    elif num == 7:
        ch = 'H'
    elif num == 8:
        ch = 'I'
    elif num == 9:
        ch = 'K'
    elif num == 10:
        ch = 'L'
    elif num == 11:
        ch = 'M'
    elif num == 12:
        ch = 'N'
    elif num == 13:
        ch = 'P'
    elif num == 14:
        ch = 'Q'
    elif num == 15:
        ch = 'R'
    elif num == 16:
        ch = 'S'
    elif num == 17:
        ch = 'T'
    elif num == 18:
        ch = 'V'
    elif num == 19:
        ch = 'W'
    elif num == 20:
        ch = 'Y'

    return ch


# preparation of data
def dataset_preparation(data):

    # basic cleanup
    corpus = data.split("\n")

    y=[]
    val=[]
    for i in range(len(corpus)):
        x = []
        if (i + 1) % 5 == 0:
            for j in range(len(corpus[i])):
                x.append(charToNum(corpus[i][j]))
            val.append(x)
        else:
            for j in range(len(corpus[i])):
                x.append(charToNum(corpus[i][j]))
            y.append(x)
    corpus = y
  
    # create input sequences using list of tokens
    input_sequences = []
    for line in corpus:
        token_list = split(line)
        for i in range(1, max_sequence_len):
            n_gram_sequence = token_list[:i + 1]
            input_sequences.append(n_gram_sequence)
    three_gram_seq=[]
    val_input_sequences=[]
    for line in val:
        val_token_list = split(line)
        for i in range(1, max_sequence_len):
            val_n_gram_sequence = val_token_list[:i + 1]
            val_input_sequences.append(val_n_gram_sequence)
            if i < (max_sequence_len-2)and(len(val_token_list)>i+1):
                three_gram_seq.append([val_token_list[i-1],val_token_list[i],val_token_list[i+1]])


    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))
    val_input_sequences = np.array(pad_sequences(val_input_sequences, maxlen=max_sequence_len, padding='pre'))

    total_words=21
    # create predictors and label
    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]
    val_predictors, val_label = val_input_sequences[:, :-1], val_input_sequences[:, -1] 

    label = ku.to_categorical(label, num_classes=total_words)
    val_label = ku.to_categorical(val_label, num_classes=total_words)


    return predictors, label, val_predictors, val_label, max_sequence_len, total_words, three_gram_seq

# creation of rnn-lstm model
def create_model(max_sequence_len, total_words):
    model = Sequential()
    model.add(Embedding(total_words, 10, input_length=max_sequence_len - 1))
    model.add(LSTM(150, return_sequences=True))
    model.add(Dropout(0.2))
    model.add(LSTM(100))
    model.add(Dense(total_words, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# three-gram probability calculated as value
def  three_gram_pred(three_gram_seq):
    tgs_value=[]
    tgs=np.asarray(three_gram_seq)
    for i in tgs:
        count=0
        for j in tgs:
            if np.array_equal(i,j):
                count=count+1
        tgs_value.append(float(count)/((tgs.shape[0])/max_sequence_len))
    return tgs, np.asarray(tgs_value)

# this function is used to generate sequences
def generate_text(seed_text, next_words, max_sequence_len):
    strpredicted = ''
    for _ in range(next_words):
        token_list = split(seed_text)

        x = []
        for i in range(len(token_list)):
            x.append(charToNum(token_list[i]))
        token_list=x

        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')
        predicted = model.predict_classes(token_list, verbose=0)
        strpredicted += NumToChar(predicted)
        seed_text = strpredicted
    return seed_text


data = open('file.txt').read()
predictors, label,  val_predictors, val_label, max_sequence_len, total_words, three_gram_seq = dataset_preparation(data)
model = create_model( max_sequence_len, total_words)
history = model.fit(predictors, label, validation_data=(val_predictors, val_label), epochs=60, verbose = 0) 
tgs,tgs_value=three_gram_pred(three_gram_seq)

# calculating perplexity

perp=history.history['loss']
pp=[]
for i in perp:
    pp.append(K.pow(np.e, i))

vperp=history.history['val_loss']
vpp=[]
for i in vperp:
    vpp.append(K.pow(np.e, i))


# # plotting accuracy and epochs
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy vs Epochs')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

# # plotting loss and epochs
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss vs Epochs')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()



# # plotting perplexity and epochs
plt.plot(pp)
plt.plot(vpp)
plt.title('Perplexity vs Epochs')
plt.ylabel('perplexity')
plt.xlabel('epoch')
plt.legend(['perplexity', 'val_perplexity'], loc='upper left')
plt.show()

#query to complete sequences
print(generate_text("D",39,max_sequence_len))

